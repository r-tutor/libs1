<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml"><head><title>R: Diagonal Discriminant Analysis</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link rel="stylesheet" type="text/css" href="R.css" />
</head><body><div class="container">

<table width="100%" summary="page for diagDA {sfsmisc}"><tr><td>diagDA {sfsmisc}</td><td style="text-align: right;">R Documentation</td></tr></table>

<h2>Diagonal Discriminant Analysis</h2>

<h3>Description</h3>

<p>This function implements a simple Gaussian maximum likelihood
discriminant rule, for diagonal class covariance matrices.
</p>
<p>In machine learning lingo, this is called &ldquo;Naive Bayes&rdquo; (for
continuous predictors).  Note that naive Bayes is more general, as it
models discrete predictors as multinomial, i.e., binary predictor
variables as Binomial / Bernoulli.
</p>


<h3>Usage</h3>

<pre>
dDA(x, cll, pool = TRUE)
## S3 method for class 'dDA'
predict(object, newdata, pool = object$pool, ...)
## S3 method for class 'dDA'
print(x, ...)

diagDA(ls, cll, ts, pool = TRUE)
</pre>


<h3>Arguments</h3>

<table summary="R argblock">
<tr valign="top"><td><code>x,ls</code></td>
<td>
<p>learning set data matrix, with rows corresponding to
cases (e.g., mRNA samples) and columns to predictor variables
(e.g., genes).</p>
</td></tr>
<tr valign="top"><td><code>cll</code></td>
<td>
<p>class labels for learning set, must be consecutive integers.</p>
</td></tr>
<tr valign="top"><td><code>object</code></td>
<td>
<p>object of class <code>dDA</code>.</p>
</td></tr>
<tr valign="top"><td><code>ts, newdata</code></td>
<td>
<p>test set (prediction) data matrix, with rows corresponding
to cases and columns to predictor variables.</p>
</td></tr>
<tr valign="top"><td><code>pool</code></td>
<td>
<p>logical flag.  If true (by default), the covariance matrices
are assumed to be constant across classes and the discriminant rule
is linear in the data.  Otherwise (<code>pool= FALSE</code>), the
covariance matrices may vary across classes and the discriminant
rule is quadratic in the data.</p>
</td></tr>
<tr valign="top"><td><code>...</code></td>
<td>
<p>further arguments passed to and from methods.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>dDA()</code> returns an object of class <code>dDA</code> for which there are
<code><a href="../../base/html/print.html">print</a></code> and <code><a href="../../stats/html/predict.html">predict</a></code> methods.  The latter
returns the same as <code>diagDA()</code>:
</p>
<p><code>diagDA()</code> returns an integer vector of class predictions for the
test set.
</p>


<h3>Author(s)</h3>


<p>Sandrine Dudoit, <a href="mailto:sandrine@stat.berkeley.edu">sandrine@stat.berkeley.edu</a>  and<br />
Jane Fridlyand, <a href="mailto:janef@stat.berkeley.edu">janef@stat.berkeley.edu</a> originally wrote
<code>stat.diag.da()</code> in CRAN package <a href="https://CRAN.R-project.org/package=sma"><span class="pkg">sma</span></a> which was modified
for speedup by Martin Maechler <a href="mailto:maechler@R-project.org">maechler@R-project.org</a>
who also introduced <code>dDA</code> etc.
</p>


<h3>References</h3>

<p>S. Dudoit, J. Fridlyand, and T. P. Speed. (2000)
Comparison of Discrimination Methods for the Classification of Tumors
Using Gene Expression Data.
(Statistics, UC Berkeley, June 2000, Tech Report \#576)
</p>


<h3>See Also</h3>

<p><code><a href="../../MASS/help/lda.html">lda</a></code> and <code><a href="../../MASS/help/qda.html">qda</a></code> from the
<a href="https://CRAN.R-project.org/package=MASS"><span class="pkg">MASS</span></a> package;
<code><a href="../../e1071/help/naiveBayes.html">naiveBayes</a></code> from <a href="https://CRAN.R-project.org/package=e1071"><span class="pkg">e1071</span></a>.
</p>


<h3>Examples</h3>

<pre>
## two artificial examples by Andreas Greutert:
d1 &lt;- data.frame(x = c(1, 5, 5, 5, 10, 25, 25, 25, 25, 29),
                 y = c(4, 1, 2, 4,  4,  4,     6:8,     7))
n.plot(d1)
library(cluster)
(cl1P &lt;- pam(d1,k=4)$cluster) # 4 surprising clusters
with(d1, points(x+0.5, y, col = cl1P, pch =cl1P))

i1 &lt;- c(1,3,5,6)
tr1 &lt;- d1[-i1,]
cl1. &lt;- c(1,2,1,2,1,3)
cl1  &lt;- c(2,2,1,1,1,3)
plot(tr1, cex=2, col = cl1, pch = 20+cl1)
(dd.&lt;- diagDA(tr1, cl1., ts = d1[ i1,]))# ok
(dd &lt;- diagDA(tr1, cl1 , ts = d1[ i1,]))# ok, too!
points(d1[ i1,], pch = 10, cex=3, col = dd)

## use new fit + predict instead :
(r1 &lt;- dDA(tr1, cl1))
(r1.&lt;- dDA(tr1, cl1.))
stopifnot(dd == predict(r1,  new = d1[ i1,]),
          dd.== predict(r1., new = d1[ i1,]))

plot(tr1, cex=2, col = cl1, bg = cl1, pch = 20+cl1,
     xlim=c(1,30), ylim= c(0,10))
xy &lt;- cbind(x= runif(500, min=1,max=30), y = runif(500, min=0, max=10))
points(xy, cex= 0.5, col = predict(r1, new = xy))
abline(v=c( mean(c(5,25)), mean(c(25,29))))

## example where one variable xj has  Var(xj) = 0:
x4 &lt;- matrix(c(2:4,7, 6,8,5,6,  7,2,3,1, 7,7,7,7), ncol=4)
y &lt;- c(2,2, 1,1)
m4.1 &lt;- dDA(x4, y, pool = FALSE)
m4.2 &lt;- dDA(x4, y, pool = TRUE)
xx &lt;- matrix(c(3,7,5,7), ncol=4)
predict(m4.1, xx)## gave integer(0) previously
predict(m4.2, xx)
</pre>

<hr /><div style="text-align: center;">[Package <em>sfsmisc</em> version 1.1-12 <a href="00Index.html">Index</a>]</div>
</div></body></html>
